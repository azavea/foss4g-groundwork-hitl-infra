{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aad00de",
   "metadata": {},
   "source": [
    "# Human-in-the-loop Machine Learning with GroundWork and Raster Vision\n",
    "\n",
    "This notebook will walk you through a process in which you get a base model from _somewhere_, use it to create predictions, upload those predictions to the [GroundWork](https://groundwork.azavea.com/) application, correct those predictions, and use the corrections to improve your original model. Once you can get from a model to predictions to an improved model, you can repeat that process any number of times. That's the loop. You're the human. Let's rock.\n",
    "\n",
    "A note on the environment: this notebook is written to run inside a container launched by `docker/run` on `master` in the `raster-vision` repo. If you're not sure how a dependency is available, why we didn't set things up in advance, or why we're in Python 3.6, that's the reason. Additionally, the environment configured by this repository's ansible scripts makes sure that we have some specific data in a specific location. If you run this outside of that configured environment, you'll need to specificy a different path to your data.\n",
    "\n",
    "## From predictions to a GroundWork project\n",
    "\n",
    "### Step 1: Set up dependencies\n",
    "\n",
    "The `raster-vision` container has lots of things we'll need (PyTorch, the python scientific stack, a bunch of system dependencies) already available, but we'll need a few more dependencies.\n",
    "\n",
    "These additional dependencies and what we'll use them for are:\n",
    "\n",
    "- `geopandas`: to figure out which _tasks_ (more below) to put our predictions in\n",
    "- `shapely`: for transforming geojson to python data\n",
    "- `rtree`: `geopandas` needs this for spatial joins, but it's an optional dependency, so we have to say we want it\n",
    "\n",
    "Run the cell below, then restart the notebook (`0 0` or `Kernel` -> `restart`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d92d9c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%pip install geopandas shapely rtree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58e388b",
   "metadata": {},
   "source": [
    "After that, we'll import everything we're going to need over the rest of the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7806ea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import functools\n",
    "from itertools import zip_longest\n",
    "import json\n",
    "from os.path import join\n",
    "from random import random\n",
    "import time\n",
    "from uuid import uuid4\n",
    "\n",
    "import geopandas as gpd\n",
    "from geopandas.tools import sjoin\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "from scipy.special import softmax\n",
    "import shapely\n",
    "from shapely.geometry import MultiPolygon, shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb55fb34",
   "metadata": {},
   "source": [
    "### Step 2: Configuration\n",
    "\n",
    "You'll configure a few values here. The goals of this configuration are to make it so that you can talk to the Raster Foundry API (the same API that powers GroundWork) from within the notebook.\n",
    "\n",
    "The values you'll configure are:\n",
    "\n",
    "- `bearer_token`: you can get this from network tools while logged in to GroundWork. This is a JSON Web Token. To see what it represents and learn more about JSON Web Tokens, you can decode it at [jwt.io](https://jwt.io/)\n",
    "- `url_base`: this value configures the scheme and host for requests to the Raster Foundry API. All of our requests will start with `app.rasterfoundry.com`\n",
    "- `source_project_id`: this value is a UUID pointing to a project template. GroundWork has two sort of high level grouping concepts -- a _project_ is a specific image that you'd like to do labeling work in, and a _campaign_ is a group of projects. Since we have predictions over an image, we want to work at the project level in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95025aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your token for interacting with the Raster Foundry API\n",
    "bearer_token = \"your-token-here\"\n",
    "\n",
    "# common HTTP headers shared by the requests we're going to make\n",
    "headers = {\"Authorization\": f\"Bearer {bearer_token}\"}\n",
    "\n",
    "# The base URL for the Raster Foundry API\n",
    "# This will only be different if you're working with a copy of Raster Foundry that lives somewhere else\n",
    "url_base = \"https://app.rasterfoundry.com\"\n",
    "\n",
    "# UUID of your template project\n",
    "source_project_id = \"your-source-project\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fc95c6",
   "metadata": {},
   "source": [
    "### Step 3: Create a copy of your template project\n",
    "\n",
    "The workflow we'll use in this workshop is that each iteration of the human-in-the-loop workflow will create a new project based on the template that we configured above. The steps to copy a project are:\n",
    "\n",
    "- fetch the existing project\n",
    "- create a new JSON document like from fields in that project\n",
    "- POST that new project to the Raster Foundry API\n",
    "- fetch all the tasks in the existing project\n",
    "- add them to the new project by changing their project reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf37451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the source project\n",
    "@functools.lru_cache(None)\n",
    "def fetch_project(project_id):\n",
    "    return requests.get(join(url_base, \"api\", \"annotation-projects\", \n",
    "                             project_id), headers=headers).json()\n",
    "\n",
    "# create a new JSON document from the source project\n",
    "def make_project_clone_json(source_project, iteration_number=0):\n",
    "    return {\n",
    "        # the name is just the source project name with a \"_HITL\" suffix\n",
    "        \"name\": f\"\"\"{source_project[\"name\"]} {iteration_number}\"\"\",\n",
    "        \"projectType\": source_project[\"projectType\"],\n",
    "        \"taskSizePixels\": 512,\n",
    "        \"aoi\": source_project[\"aoi\"],\n",
    "        \"labelersTeamId\": source_project[\"labelersTeamId\"],\n",
    "        \"validatorsTeamId\": source_project[\"validatorsTeamId\"],\n",
    "        \"projectId\": None,\n",
    "        \"campaignId\": source_project[\"campaignId\"],\n",
    "        \"status\": source_project[\"status\"],\n",
    "        \"tileLayers\": source_project[\"tileLayers\"],\n",
    "        \"labelClassGroups\": []\n",
    "    }\n",
    "\n",
    "# post the project copy to the Raster Foundry API\n",
    "def post_project(project_json):\n",
    "    post_hitl_url = join(url_base, \"api\",\"annotation-projects\")\n",
    "    post_hitl = requests.post(post_hitl_url, headers=headers, json=project_json)\n",
    "    post_hitl.raise_for_status()\n",
    "    return post_hitl.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea83ecd",
   "metadata": {},
   "source": [
    "So we can get the source project by bundling that workflow up with `project_id` and `iteration_number` parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187a6ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_project(project_id, iteration_number):\n",
    "    source_project = fetch_project(project_id)\n",
    "    new_project = make_project_clone_json(source_project, iteration_number)\n",
    "    return post_project(new_project)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de6bb9f",
   "metadata": {},
   "source": [
    "We can then make our first copy of the template project like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1e3801",
   "metadata": {},
   "outputs": [],
   "source": [
    "hitl_project = clone_project(source_project_id, 1)\n",
    "f\"\"\"https://groundwork.azavea.com/app/campaign/{hitl_project[\"campaignId\"]}/overview?p=0&f=all\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70c43f8",
   "metadata": {},
   "source": [
    "You can visit that URL in the GroundWork app and select the project you just created to see that we've created a copy of the project that has the same imagery tile layer, but it doesn't appear to have any tasks.\n",
    "\n",
    "#### Uploading tasks\n",
    "\n",
    "GroundWork breaks labeling work into more manageable pieces called _tasks_. A task is a specific window within a larger image. If you look at the overview for the source project, each little box over the imagery is a task.\n",
    "\n",
    "To fill in tasks, we'll need to:\n",
    "\n",
    "* grab all of the tasks from the source project\n",
    "* POST them to the new project\n",
    "\n",
    "Since the number of tasks in the source project can be huge, we'll work in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcabf6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab all of the tasks from the source project\n",
    "def fetch_tasks(annotation_project_id, url_base):\n",
    "    template_project_tasks_url = join(url_base,\"api/annotation-projects/\", annotation_project_id, \"tasks\")\n",
    "    tasks = requests.get(template_project_tasks_url, headers=headers).json()\n",
    "    has_next = tasks[\"hasNext\"]\n",
    "    next_page = 1\n",
    "    while has_next:\n",
    "        new_tasks_url = f\"{template_project_tasks_url}?page={next_page}\"\n",
    "        next_tasks = requests.get(new_tasks_url, headers=headers).json()\n",
    "        tasks[\"features\"] += next_tasks[\"features\"]\n",
    "        has_next = next_tasks[\"hasNext\"]\n",
    "        next_page += 1\n",
    "    return tasks\n",
    "\n",
    "# break all tasks into manageable chunks\n",
    "# modified from https://docs.python.org/3/library/itertools.html#itertools-recipes\n",
    "def grouper(iterable, n):\n",
    "    \"Collect data into fixed-length chunks or blocks\"\n",
    "    # grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx\"\n",
    "    args = [iter(iterable)] * n\n",
    "    x = zip_longest(*args)\n",
    "    # workaround to remove the fill values in the chunks\n",
    "    return [[ii for ii in i if ii is not None] for i in x]\n",
    "\n",
    "# POST the tasks to the new project in groups\n",
    "def copy_tasks_to_project(source_tasks, project_id):\n",
    "    tasks_post_url = join(url_base, \"api\", \"annotation-projects\", project_id, \"tasks\")\n",
    "    chunks = grouper(source_tasks['features'], 1250)\n",
    "    out = {\"type\": \"FeatureCollection\", \"features\": []}\n",
    "    for chunk in chunks:\n",
    "        chunk_tasks = {\"features\": [], \"type\": \"FeatureCollection\"}\n",
    "        for task in chunk:\n",
    "            # set the status to unlabeled, no matter what it was before\n",
    "            task[\"properties\"][\"status\"] = \"UNLABELED\"\n",
    "            # set the annotationProjectId to \n",
    "            task[\"properties\"][\"annotationProjectId\"] = project_id\n",
    "            chunk_tasks[\"features\"] += [task]\n",
    "    \n",
    "        chunk_tasks_response = requests.post(tasks_post_url, headers=headers, json=chunk_tasks)\n",
    "        # make sure this post request doesn't fail silently\n",
    "        chunk_tasks_response.raise_for_status()\n",
    "        out[\"features\"] += chunk_tasks_response.json()[\"features\"]\n",
    "    return out\n",
    "\n",
    "# One-shot to grab all the tasks and do the copy\n",
    "def clone_tasks(from_project_id, to_project_id):\n",
    "    source_tasks = fetch_tasks(source_project_id, url_base)\n",
    "    return copy_tasks_to_project(source_tasks, to_project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dcd09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hitl_project_tasks = clone_tasks(source_project_id, hitl_project[\"id\"])\n",
    "len(hitl_project_tasks[\"features\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9f3acb",
   "metadata": {},
   "source": [
    "### Step 4: upload labels\n",
    "\n",
    "Now that we have a project and tasks, we can upload our labels. To upload the labels, we'll need to complete three steps:\n",
    "\n",
    "* associate each predicted label with a label class\n",
    "* associate each predicted label with a task\n",
    "* POST all the labels to GroundWork\n",
    "\n",
    "Each label has to be associated with a task within a project and with a label class. GroundWork associates labels with classes via UUIDs, while Raster Vision only speaks string names, so we'll need to make sure we can translate between the names and IDs. Fortunately, we can get that translation from the campaign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0515e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a dict to map from label name (from Raster Vision) to label class id (GroundWork / Raster Foundry)\n",
    "def get_class_map(project):\n",
    "    campaign_id = project['campaignId']\n",
    "    get_label_class_url = join(url_base, \"api\", \"campaigns\", campaign_id, \"label-class-groups\")\n",
    "    label_class_summary = requests.get(get_label_class_url, headers=headers).json()\n",
    "    return {d['name']: d['id'] for d in label_class_summary[0]['labelClasses']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109c516b",
   "metadata": {},
   "source": [
    "Joining labels to tasks is slightly more complex. Our labels in this case are chip classification labels. Those chips won't align perfectly with the task grid that we created. However, both the tasks and the chips happen to be square, so we know that if the centroid of a label is located within a task, that's the most appropriate task for the label. We can do this kind of join with geopandas. We'll also need to track the predictions' original geometries though, so we'll return that in a separate map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667a9164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def associate_tasks(predictions_feature_collection, tasks_feature_collection):\n",
    "    geom_mapping = {}\n",
    "    tasks_df = gpd.GeoDataFrame.from_features(tasks_feature_collection[\"features\"], crs=\"epsg:4326\")\n",
    "    copied = deepcopy(predictions_feature_collection)\n",
    "    for f in copied[\"features\"]:\n",
    "        f[\"properties\"][\"score\"] = np.max(softmax(f[\"properties\"][\"scores\"]))\n",
    "        geom = shape(f[\"geometry\"])\n",
    "        ad_hoc_id = str(uuid4())\n",
    "        geom_mapping[ad_hoc_id] = geom\n",
    "        f[\"properties\"][\"ad-hoc-id\"] = ad_hoc_id\n",
    "        # find the centroids which we will use for easier joining to task grid\n",
    "        f[\"geometry\"] = geom.centroid\n",
    "    return {\"original_geometries\": geom_mapping,\n",
    "            \"joined\": sjoin(\n",
    "                tasks_df,\n",
    "                gpd.GeoDataFrame.from_features(copied['features'], crs='EPSG:4326'),\n",
    "                how=\"left\"\n",
    "            )}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bb7188",
   "metadata": {},
   "outputs": [],
   "source": [
    "rv_output_uri = \"base-predictions.json\"\n",
    "\n",
    "with open(rv_output_uri, \"r\") as inf:\n",
    "    predictions_feature_collection = json.load(inf)\n",
    "\n",
    "labels_with_task_ids = associate_tasks(predictions_feature_collection, hitl_project_tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da278c7f",
   "metadata": {},
   "source": [
    "Finally, we can post these predictions to GroundWork:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d7ce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the json needed to post labels from each row in the labels with task ID table\n",
    "def features_to_label_post_body(group, geom_mapping, class_map):\n",
    "    def feature_to_label(r):\n",
    "        try:\n",
    "            invert = r[\"class_name\"] == \"background\"\n",
    "            return { \"type\": \"Feature\",\n",
    "              \"properties\": {\n",
    "                \"annotationLabelClasses\": [class_map[\"boat\"]],\n",
    "                \"score\": r[\"score\"] if not invert else 1 - r[\"score\"]\n",
    "              },\n",
    "              \"geometry\": shapely.geometry.mapping(geom_mapping[r[\"ad-hoc-id\"]]),\n",
    "             \"id\": r[\"id\"]\n",
    "            }\n",
    "        except:\n",
    "            print(\"Failing row:\")\n",
    "            print(r)\n",
    "            raise\n",
    "\n",
    "    return {\n",
    "      \"type\":\"FeatureCollection\",\n",
    "      \"features\": [feature_to_label(r) for _, r in group.iterrows() if not gpd.pd.isna(r['class_name'])],\n",
    "        \"nextStatus\":\"LABELED\"\n",
    "    }\n",
    "\n",
    "def upload_labels(project_id, joined, original_geometries, class_map):\n",
    "    for task_id, task_labels in joined.groupby(\"id\"):\n",
    "        label_upload_body = features_to_label_post_body(task_labels, original_geometries, class_map)\n",
    "        label_upload_url = join(url_base, \"api\", \"annotation-projects\", project_id, \"tasks\", task_id, \"labels\")\n",
    "        # we use a PUT here so that if we fail in the middle, we can try again and replace the labels\n",
    "        label_upload_response = requests.put(label_upload_url, headers=headers, json=label_upload_body)\n",
    "        # make sure this post request doesn't fail silently\n",
    "        label_upload_response.raise_for_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e074accc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = get_class_map(hitl_project)\n",
    "upload_labels(hitl_project[\"id\"], class_map=class_map, **labels_with_task_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5b5120",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "The above steps are all separated out, but we can instead write a single function that runs through the entire workflow like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935f715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rv_label_project(source_project_id, iteration_number, rv_output_uri):\n",
    "    hitl_project = clone_project(source_project_id, iteration_number)\n",
    "    hitl_project_tasks = clone_tasks(source_project_id, hitl_project[\"id\"])\n",
    "    \n",
    "    with open(rv_output_uri, \"r\") as inf:\n",
    "        predictions_feature_collection = json.load(inf)\n",
    "    labels_with_task_ids = associate_tasks(predictions_feature_collection, hitl_project_tasks)\n",
    "    class_map = get_class_map(hitl_project)\n",
    "    upload_labels(hitl_project[\"id\"], class_map=class_map, **labels_with_task_ids)\n",
    "    return hitl_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f2d7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can skip this for now unless you want to create another project\n",
    "create_rv_label_project(source_project_id, 1, rv_output_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51c0b08",
   "metadata": {},
   "source": [
    "We'll return to this step later when we run through the loop again.\n",
    "\n",
    "## From GroundWork to new training data\n",
    "\n",
    "Before, we set a task status of `UNLABELED` when we created tasks, then `LABELED` when we uploaded labels to them. There's a more advanced status, `VALIDATED`, that indicates that a human has reviewed some labels and signed off on them. We can use the validation process in GroundWork to correct the predictions produced by Raster Vision.\n",
    "\n",
    "The validation workflow uses GroundWork. After you're done with that, come back here.\n",
    "\n",
    "...\n",
    "\n",
    "...\n",
    "\n",
    "...\n",
    "\n",
    "Done validating? Great. Let's create some new training data.\n",
    "\n",
    "To do that, we'll create a STAC export. STAC is short for the [spatio-temporal asset catalog specification](https://github.com/radiantearth/stac-spec), an open, extensible standard for describing geospatial data. GroundWork knows how to export, and Raster Vision knows how to train models from, STAC catalogs implementing the [label extension](https://github.com/stac-extensions/label). We can create a new export using the `/stac` endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286b7c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validated_stac_export(campaign_id):\n",
    "    export_post = {\n",
    "        \"name\": \"GW HITL Workshop export\",\n",
    "        \"license\": {\"license\": \"proprietary\"},\n",
    "        \"taskStatuses\": [\"VALIDATED\"],\n",
    "        \"exportAssetType\": None,\n",
    "        \"campaignId\": campaign_id\n",
    "    }\n",
    "    resp = requests.post(f\"{url_base}/api/stac\", headers=headers, json=export_post)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b443af",
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign_id = \"9ed79bc2-dafa-4fa9-a4f8-0bce214d070e\"\n",
    "export = create_validated_stac_export(campaign_id)\n",
    "export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8927daa",
   "metadata": {},
   "source": [
    "We can wait for the export to complete within the notebook by checking its status and sleeping -- exports don't take very long so this won't run too many times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d2c578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_export(export):\n",
    "    export = requests.get(f\"\"\"{url_base}/api/stac/{export[\"id\"]}\"\"\", headers=headers).json()\n",
    "    if export[\"exportStatus\"] != \"EXPORTED\":\n",
    "        time.sleep(10)\n",
    "        return wait_for_export(export)\n",
    "    return export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9285be",
   "metadata": {},
   "outputs": [],
   "source": [
    "completed = wait_for_export(export)\n",
    "download_url = completed[\"downloadUrl\"]\n",
    "download_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093982b7",
   "metadata": {},
   "source": [
    "Similarly to before, we can create a single function that creates and waits for the export, then prints the donwload URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dfe598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_campaign(campaign_id):\n",
    "    export = create_validated_stac_export(campaign_id)\n",
    "    completed = wait_for_export(export)\n",
    "    return completed[\"downloadUrl\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c88b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_campaign(campaign_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14d6d57",
   "metadata": {},
   "source": [
    "We can download and unzip that export easily with bash:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c332e0ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This iteration refers to the next training round number\n",
    "iter_num = 2\n",
    "\n",
    "# Last iteration, from the perspective of the next training round, is iter_num - 1\n",
    "last_iter = iter_num - 1\n",
    "\n",
    "!mkdir -p export-data/iter-{last_iter}\n",
    "!wget -O export-data/iter-{last_iter}/stac-export.zip \"{download_url}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2dffdc",
   "metadata": {},
   "source": [
    "To see what's in the export, we can unzip it and then explore it using the notebook server's file browser. Start with `README.md`. Now we have new training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42d1ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -d export-data/iter-{last_iter}/ export-data/iter-{last_iter}/stac-export.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb48ed64",
   "metadata": {},
   "source": [
    "## From new training data to new predictions\n",
    "\n",
    "Now that we have new training data, we can train a new model based on our original model. We can load our original model from a file containing its weights. Again, this step starts with some configuration. We'll configure a few values that control how the Raster Vision training works:\n",
    "\n",
    "- `MODEL_INIT_WEIGHTS_PATH`: where the pre-trained model weights live\n",
    "- `OUTPUT_ROOT`: the directory that will serve as the base for next training runs\n",
    "- `RV_CONFIG_FILE`: a python module containing a `get_config` function that returns a Raster Vision experiment config\n",
    "- `stac_export_uri`: the downloaded location of the zip containing the STAC export we created before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f34452-7d25-411a-96a0-207f2d4f7579",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_INIT_WEIGHTS_PATH = \"./init_weights.pth\"\n",
    "OUTPUT_ROOT = \"./output\"\n",
    "RV_CONFIG_FILE = \"./active_learning.py\"\n",
    "\n",
    "output_dir = f\"{OUTPUT_ROOT}/iter_{iter_num}\"\n",
    "stac_export_uri = f\"./export-data/iter-{last_iter}/stac-export.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8ebda3",
   "metadata": {},
   "source": [
    "With those values configured, we can re-train our original model with the new training data we just exported. The `rastervision run inprocess` command runs Raster Vision locally with the configured commands and arguments. In a production environment you'd probably run this process for more epochs and with a GPU, but here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d04f2c9-1c3f-4bf7-b492-b45122fe0fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rastervision run inprocess \"{RV_CONFIG_FILE}\" \\\n",
    "    train predict \\\n",
    "    -a output_dir \"{output_dir}\" \\\n",
    "    -a num_epochs \"3\" \\\n",
    "    -a chips_per_scene \"100\" \\\n",
    "    -a stac_export_uri \"{stac_export_uri}\" \\\n",
    "    -a init_weights \"{MODEL_INIT_WEIGHTS_PATH}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dffc24-2052-4a66-ae02-a5a52c66b449",
   "metadata": {},
   "source": [
    "### Inspect predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f29895-1838-45d1-8aed-15b99472bab3",
   "metadata": {},
   "source": [
    "Now we have some new predictions -- we can see the distribution of predicted scores for whether a chip contains a boat like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b050a4-49d4-4967-ba1f-48ce575ac0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{output_dir}/predict/val-0.json\", \"r\") as f:\n",
    "    preds = json.load(f)\n",
    "\n",
    "scores_boat = np.array([softmax(f['properties']['scores'])[1] for f in preds['features']])\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.hist(scores_boat, bins=30, alpha=0.5, label='boat')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fb8b59",
   "metadata": {},
   "source": [
    "What do we do with those predictions? Well, we have a nice little workflow for getting them into GroundWork, and then to correct them and turn them into new training data. A shorter version of the workflow can be seen below --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6704bd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new project\n",
    "\n",
    "# increment this value each time you create a new project\n",
    "iter_num = 2\n",
    "\n",
    "rv_output_uri = f\"./output/iter_{iter_num - 1}/predict/val-0.json\"\n",
    "\n",
    "next_project = create_rv_label_project(source_project_id, iter_num, rv_output_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d700b6bd",
   "metadata": {},
   "source": [
    "Then go validate some labels, then create a new export:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9254abe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_training_url = export_campaign(next_project[\"campaignId\"])\n",
    "next_training_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1448de",
   "metadata": {},
   "source": [
    "Finally, download the new training data, and re-run training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6fd36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"{OUTPUT_ROOT}/iter_{iter_num}\"\n",
    "stac_export_uri = f\"./export-data/iter-{iter_num}/stac-export.zip\"\n",
    "\n",
    "!mkdir -p export-data/iter-{iter_num}\n",
    "!wget -O export-data/iter-{iter_num}/stac-export.zip \"{next_training_url}\"\n",
    "!rastervision run inprocess \"{RV_CONFIG_FILE}\" \\\n",
    "    train predict \\\n",
    "    -a output_dir \"{output_dir}\" \\\n",
    "    -a num_epochs \"3\" \\\n",
    "    -a chips_per_scene \"100\" \\\n",
    "    -a stac_export_uri \"{stac_export_uri}\" \\\n",
    "    -a init_weights \"{MODEL_INIT_WEIGHTS_PATH}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344f84ee",
   "metadata": {},
   "source": [
    "You can re-run the above three cells with the validation step in the middle as many times as you want to improve the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
